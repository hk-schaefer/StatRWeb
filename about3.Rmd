---
title: "Project 3: Step-by-Step Guide"
output: html_document
---

# LOGISTIC REGRESSION

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r load-packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(statsr)
library(GGally)          # ------------------- extension of ggplot2
library(patchwork)       # ------------------- additional visualization tool
library(broom)           # ------------------- Convert stat obj. into tibbles
library(knitr)           # ------------------- support print of tables

```
\

```{r}
variables <- read.csv("~/R/StatRWeb/data/movies_variables_logit.csv", header = TRUE)

# Training data
train0 <- readRDS("~/R/StatRWeb/data/train0.Rdata")
train_model <- readRDS("~/R/StatRWeb/data/train_model.Rdata")
train_model2 <- readRDS("~/R/StatRWeb/data/train_model2.Rdata")

# Test data
test0 <- readRDS("~/R/StatRWeb/data/test0.Rdata")
test1 <- readRDS("~/R/StatRWeb/data/test1.Rdata")

# Model
model_final <- readRDS("~/R/StatRWeb/data/model_final.Rdata")

# Classification results
class_metrics <- readRDS("~/R/StatRWeb/data/class_metrics.Rdata")

# Support files
probs1 <- readRDS("~/R/StatRWeb/data/probs1.Rdata")
probs2 <- readRDS("~/R/StatRWeb/data/probs2.Rdata")
probs3 <- readRDS("~/R/StatRWeb/data/probs3.Rdata")
```



\
\
```{r fig.align='center', fig.show='hold', out.width="30%"}
knitr::include_graphics(c("~/R/StatRWeb/images/02_icon.png")
)
```
\


## Introduction


**Objectives**

The objective of this project is to apply learned contents of "Logistic Regression and Modeling". Skills required for this project are:\


`Logistic Regression`, `Exploratory Data Analysis`, `Variable Selection`, `Modeling`, `Interpretation`, `Prediction`, `Classification`, `Statistical Thinking`, `R programming`\
\


**Contents**

The logistic regression modeling project was split into the sections:

- Part 1: Data inspections
- Part 2: Research question
- Part 3: Explanatory data analysis
- Part 4: Modeling
- Part 5: Prediction

Code chunks and detailed descriptions can be found in the [**R-Markdown**](project3.html) file.
\
\

**Project Instruction**

This project is a non-guided project and was not part of the course [**Data Analysis with R, Duke University**](https://www.coursera.org/specializations/statistics) hosted on the Coursera platform. The entire project was carried out by myself, from problem definition to answering the question. For this purpose, a multiple logistic regression model was implemented and tested in R.

In project [Linear Regression](about2.html) we analyzed a movie dataset and answered the question what factors make a movie popular. The variable of interest was a continuous variable `imdb_rating` and we used the **multiple linear regression** method to answer this problem.

For this project we modified the research question in a way that we must apply a **logistic regression** model. We, therefore, selected as the variable of interest the binary variable `audience_rating`.

As part of this project, we completed exploratory data analysis (EDA), modeling, and prediction.\
\
\
\

## 1. Data inspection

The dataset was provided by the course [**Data Analysis with R, Duke University**](https://www.coursera.org/specializations/statistics) hosted on the Coursera platform. The dataset includes information from [Rotten Tomatos](https://www.rottentomatoes.com/) and [IMDB](https://www.imdb.com/).\

The dataset used here is a sample of 651 observations with 8 variables. The data were collected in the years from 1970 to 2014 from the audience on voluntary basis and from selected groups of movie critics. So we can't claim that they were collected by random sampling. We, therefore, cannot infer the statistics to the general population, i.e. movie popularity for all people in the US. We can only infer the model on new data collected in the same time frame and under the same method as our sample dataset.
\
\
\

## 2. Research question

1. Can the binary (audience) rating be explained by factors such as genre, other rating measures (e.g, IMDb and critics ratings), or nomination and awards (e.g. Academy Award-winning films)?\

2. What are the most influential predictors for audience rating?\
\


**Task**

Build a logistic regression model to best fit the relationships between these variables optimized for a high prediction accuracy for new data.

The logistic regression model has the following linear form:

\[
  \begin{aligned}
  log(\frac{p}{1-p}) = \beta{_0}\ + \beta{_1} x{_1} + \beta{_2} x{_2} + \beta{_3} x{_3} + . . . + \beta{_k} x{_k}
  \end{aligned}
\]

Where:\

- A success is defined if outcome `audience_rating` = "Upright" or "1".\
- $p$ is the probability of a success\ 
- $odd = \frac{p}{1-p}$ is the probability of success vs the probability of failure\
- $logit(p)$ is the logit function of $p$ equal to $log(\frac{p}{1-p})$
- $\beta_i$ are the unknowns of the model\
- $x_i$ are the independent variables.\
\
\
\

## 3. Exploratory Data Analysis (EDA)

**The variables**

We identified seven potential independent variables of interest to be included in the modeling .\
\

Variable type                       | Variable name                   | Data type
------------------------------------|---------------------------------|------------- 
Dependent variable (response)       | audience_rating                 | categorical
                                    |                                 |
Independent variables (predictor)   | imdb_rating                     | numerical
Independent variables (predictor)   | critics_score                   | numerical
Independent variables (predictor)   | critics_rating                  | categorical
Independent variables (predictor)   | genre                           | categorical
Independent variables (predictor)   | mpaa_rating                     | categorical
Independent variables (predictors)  | best_pic_nom, best_pic_win      | categorical  

\
\

**Split data into training and test data**

For modeling (training) and testing (prediction) we split by the dataset into two sub-sets by stratified sampling:\

- Training data (80%) for model training
- Test data (20%) to check the prediction accuracy of the final model.\

\

**audience_rating distribution (training and test data)**

```{r message=FALSE, warning=FALSE}
sum_AR <- train0 %>% 
  mutate(audience_rating = as.factor(ifelse
                                     (audience_rating == 0, "Spilled (0)", "Upright (1)"))) %>% 
  group_by(audience_rating) %>% 
  summarise(n = n()) %>%
  mutate(percentage = round(n/sum(n)*100,1))

sum_AR_test <- test0 %>% 
  mutate(audience_rating = as.factor(ifelse
                                     (audience_rating == 0, "Spilled (0)", "Upright (1)"))) %>% 
  group_by(audience_rating) %>% 
  summarise(n = n()) %>%
  mutate(percentage = round(n/sum(n)*100,1))

plot1 <- train0 %>% 
  ggplot(aes(x = as.factor(audience_rating), fill = as.factor(audience_rating)))+
           geom_bar()+
  labs(title = "Distribution Audience Rating",
       subtitle = "Training Data",
       fill = "Audience Rating")+
  ylab("Number of rated movies")+
  coord_cartesian(ylim = c(1, 330))+
  scale_fill_discrete(labels = c("Spilled (0)", "Upright (1)"))+
  annotate("text", x=1, y=237, label=paste("n = ", sum_AR[1,2]), size = 3.2)+
  annotate("text", x=2, y=315, label=paste("n = ", sum_AR[2,2]), size = 3.2)+
  annotate("text", x=1, y=150, label=paste(sum_AR[1,3], "%"), size = 3.2)+
  annotate("text", x=2, y=150, label=paste(sum_AR[2,3], "%"), size = 3.2)+
  xlab("Audience Rating")+
  theme(legend.position = "none")+
  theme(plot.title = element_text(size=12))

plot2 <- test0 %>% 
  ggplot(aes(x = as.factor(audience_rating), fill = as.factor(audience_rating)))+
           geom_bar()+
  labs(title = "",
       subtitle = "Test Data",
       fill = "Audience Rating")+
  ylab("")+
  coord_cartesian(ylim = c(1, 330))+
  scale_fill_discrete(labels = c("Spilled (0)", "Upright (1)"))+
  annotate("text", x=1, y=67, label=paste("n = ", sum_AR_test[1,2]), size = 3.2)+
  annotate("text", x=2, y=90, label=paste("n = ", sum_AR_test[2,2]), size = 3.2)+
  annotate("text", x=1, y=30, label=paste(sum_AR_test[1,3], "%"), size = 3.2)+
  annotate("text", x=2, y=30, label=paste(sum_AR_test[2,3], "%"), size = 3.2)+
  xlab("Audience Rating")

plot1 + plot2
rm(plot1, plot2, sum_AR, sum_AR_test)
```
\

The proportions of "Spilled (0)" versus "Upright (1)" in both datasets are roughly evenly distributed, which is important to avoid any bias between model fitting and final testing.\
\
\

#### Relationships between dependent and selected independent variables

**`audience_rating` vs `imdb_rating`**

```{r message=FALSE, warning=FALSE}

plot1 <- train0 %>% 
  mutate(audience_rating2 = ifelse(audience_rating == 0, "Spilled (0)", "Upright (1)")) %>% 
  ggplot(aes(x = imdb_rating)) +
  geom_boxplot()+
  labs(title = "",
       subtitle = "Spread")+
  facet_wrap(~as.factor(audience_rating2), nrow = 2)+
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank())+
  ylab("")
  
plot2 <- train0 %>% 
  mutate(audience_rating2 = ifelse(audience_rating == 0, "Spilled (0)", "Upright (1)")) %>% 
  ggplot(aes(x = imdb_rating))+
  geom_histogram(fill = "darkgrey", color = "black")+
  labs(title = "IMDb Rating by Audience Rating",
       subtitle = "Distribution (training data)")+
  facet_wrap(~as.factor(audience_rating2), nrow = 2)+
  ylab("")

plot2 + plot1
```
\
Movies with lower IMDb ratings are more likely to be rated as "Spilled" and higher IMDd ratings are more likely to be rated as "Upright". This variable is of interest.
\
\

**`audience_rating` vs `genre`**

```{r}
sum_spilled_ge <- train0 %>% 
  mutate(audience_rating = as.factor(ifelse(audience_rating == 0, "Spilled (0)", "Upright (1)"))) %>% 
  filter(audience_rating == "Spilled (0)") %>% 
  group_by(genre) %>% 
  summarise(n = n()) %>%
  mutate(percentage = round(n/sum(n)*100,1))

# sum(sum_spilled_ge$n)

sum_upright_ge <- train0 %>% 
  mutate(audience_rating = as.factor(ifelse(audience_rating == 0, "Spilled (0)", "Upright (1)"))) %>% 
  filter(audience_rating == "Upright (1)") %>% 
  group_by(genre) %>% 
  summarise(n = n()) %>%
  mutate(percentage = round(n/sum(n)*100,1))

# sum(sum_upright_ge$n)

plot1 <- train0 %>% 
  ggplot(aes(x = fct_rev(genre))) +
  geom_bar(position = "fill", aes(fill = as.factor(audience_rating)))+
  labs(title = "Audience Rating by Genre",
       subtitle = "Distribution (training data)",
       fill = "Audience Rating")+
  scale_fill_discrete(labels = c("Spilled (0)", "Upright (1)"))+
  xlab("")+
  ylab("proportion")+
  theme(legend.position = "none")+
  coord_flip()

plot2 <- train0 %>% 
  ggplot(aes(x = fct_rev(genre))) +
  geom_bar(aes(fill = as.factor(audience_rating)))+
  labs(fill = "")+
  scale_fill_discrete(labels = c("Spilled (0)", "Upright (1)"))+
  xlab("")+
  ylab("count")+
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank())+
  theme(legend.position = "bottom")+
  coord_flip()

plot1 + plot2
rm(plot1, plot2, sum_spilled_ge, sum_upright_ge)
```

\
The proportions of audience rating are distinct for each type of genre. The variable may be influential and is of interest.
\
\

**`audience_rating` vs `critics_rating`**

```{r}
sum_spilled_cr <- train0 %>% 
  mutate(audience_rating = as.factor(ifelse(audience_rating == 0, "Spilled (0)", "Upright (1)"))) %>% 
  filter(audience_rating == "Spilled (0)") %>% 
  group_by(critics_rating) %>% 
  summarise(n = n()) %>%
  mutate(percentage = round(n/sum(n)*100,1))
# sum(sum_spilled_ge$n)

sum_upright_cr <- train0 %>% 
  mutate(audience_rating = as.factor(ifelse(audience_rating == 0, "Spilled (0)", "Upright (1)"))) %>% 
  filter(audience_rating == "Upright (1)") %>% 
  group_by(critics_rating) %>% 
  summarise(n = n()) %>%
  mutate(percentage = round(n/sum(n)*100,1))
# sum(sum_upright_ge$n)

plot1 <- train0 %>% 
  ggplot(aes(x = fct_rev(critics_rating))) +
  geom_bar(position = "fill", aes(fill = as.factor(audience_rating)))+
  labs(title = "Audience Rating by Critics Rating",
       subtitle = "Distribution (training data)",
       fill = "Audience Rating")+
  scale_fill_discrete(labels = c("Spilled (0)", "Upright (1)"))+
  xlab("")+
  ylab("proportion")+
  theme(legend.position = "none")+
  coord_flip()

plot2 <- train0 %>% 
  ggplot(aes(x = fct_rev(critics_rating))) +
  geom_bar(aes(fill = as.factor(audience_rating)))+
  labs(fill = "")+
  scale_fill_discrete(labels = c("Spilled (0)", "Upright (1)"))+
  xlab("")+
  ylab("count")+
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank())+
  theme(legend.position = "bottom")+
  coord_flip()

plot1 + plot2
rm(plot1, plot2, sum_upright_cr, sum_spilled_cr)
```
\
The proportions of audience ratings are closely related to the critics ratings. The variable is influential.\
\
\


#### Check the assumption for logistic regression

**Assumption for logistic regression**
\

In order to apply logistic regression the variables must meet the following 5 assumptions:\

1. Dependent variable is binary (1 or 0)
2. The data are independent. I.e not paired, not depending on order of selection.
3. The independent variables should not correlate too strongly with each other (collinearity assumption)
4. The independent numerical variables are linearly correlated to the log odds of the dependent variable 
5. There should be no outliers
\
\

*Assumption 1:* Dependent variable is binary **is met**. (binary response variable)\
\
*Assumption 2:* Data are independent of each other **is met**. (random sample)\
\
*Assumption 3:* Independent variables are not too strongly correlated **is met** after removal of a highly correlated variable.\

The correlation coefficients between the independent variables:

```{r message=FALSE, warning=FALSE}
# Convert xi of type factors into numerical numbers 1, 2,...
# and remove response variable from data frame
train1 <- train0 %>% 
  mutate(genre = as.numeric(genre),
         mpaa_rating = as.numeric(mpaa_rating),
         critics_rating = as.numeric(critics_rating),
         best_pic_nom = as.numeric(best_pic_nom),
         best_pic_win = as.numeric(best_pic_win)) %>% 
  select(-ID,
         -audience_rating)

ggpairs(train1)
```
\
Variable `critics_score` is highly correlated to two other variables and was removed.\

```{r}
# Drop correlated variable
train3 <- train0 %>% 
  select(-critics_score)
```
\
\


*Assumption 4:* Independent numerical variable is linearly correlated to the log-odds of the dependent variable **is met**.\

```{r message=FALSE, warning=FALSE}
# Plot of natural relationship 
plot1 <- train3 %>% 
  ggplot(aes(y = audience_rating, x = imdb_rating))+
  geom_jitter(height = 0.02, alpha = 0.1, size = 3)+
  geom_smooth(method = "glm",
              method.args = list(family = "binomial"), se = FALSE, color = "blue")+
    labs(title = "Logistic regression",
       subtitle = "Audience rating vs. IMDb rating")+
  xlab("imdb_rating (continuous)")+
  ylab("audience_rating (binary)")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

# Plot of log-odds and imdb_rating
plot2 <- ggplot()+
  geom_point(data = probs1, aes(x = x_median, y = logit), 
             size = 5, color = "darkgrey", alpha = 0.9)+
  geom_smooth(data = probs1, aes(x = x_median, y = logit), method = "lm", se = F)+
  labs(title = "Linearity check",
       subtitle = "Logit(prob(audience_rating)) vs. IMDb rating")+
  xlab("imdb_rating (3 buckets)")+
  ylab("Logit (observed probability)")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

plot1 + plot2
rm(plot1, plot2)
```
\
The relationship between `imdb_rating` and the observed `logit(p)` is approximately linear. 
\
\

*Assumption 5:* Outliers have been removed. The assumption **is met**. 
\

We identified and removed three outliers by Cook's distance from a simple logistic model with variable imdb_rating.\
\

```{r}
# Build logistic model with one variable 
model <- glm(audience_rating ~ imdb_rating, 
              data = train3,
              family = "binomial")
 # summary(model)
 plot(model, which = c(4))
```
\
\
\
\
\

## Part 4: Modeling

**Model selection using a function from package MASS::stepAIC()**

The model was calculated by a step-by-step forward selection approach. As measure for the model selection the function uses the AIC (Akaike Information Criterion) criteria. The lower the AIC value the better the model fit.

```{r message=FALSE, warning=FALSE}
# Build a full model
m_full <- glm(audience_rating ~ .,
              data = train_model, family = "binomial")

# Build a null model
m_null <- glm(audience_rating ~ 1, data = train_model, family = "binomial")

library(MASS)
model <- stepAIC(m_null, direction="forward", scope=list(upper = m_full,lower = m_null),trace = 0) 

detach("package:MASS", unload = TRUE)  # - remove package because masking SELECT function
# plot(model)

summary(model)
rm(m_full, m_null)
```
\

The model selected by the algorithm has 1 numerical variable and 1 categorical variable, with to levels. In total 3 variables. All variables are statistically significant.\
\
\



**Model diagnostics**

To assess the quality of the model, we plotted the predicted outcome against the true outcome by grouping the predicted probabilities into buckets (e.g. 10%). For each bucket the mid-points and confidence intervals were calculated.
\

```{r message=FALSE, warning=FALSE}
# Plot true response vs fitted values

ggplot() +
  geom_jitter(data = train_model2, aes(y = audience_rating, x = fitted), 
              height = 0.03, alpha = 0.5, size = 2, color = "darkgrey")+
  geom_abline(data = probs2, aes(x = x_median), 
              slope = 1, color = "red", size = 0.5, linetype = 6 )+
  geom_pointrange(data = probs2, aes(x = x_median, y = p, ymin = lwr, ymax = upr), size = 0.7, alpha = 0.5)+
    labs(title = "True values vs predicted values - Training Data",
       subtitle = "Observed values with 95% confidence interval (for 10 buckets)")+
  xlab("Predicted Probability")+
  ylab("Observed Probability (Truth)")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))
```
\
The points plotted should fall close to the line y = x, since the predicted probabilities should be similar to the observed probabilities. The dashed line is within the confidence bound of 95% confidence intervals for most of the buckets. We can therefore conclude that the logistic fit of the final model is reasonable.
\
\

**The final model**

In the previous chapter we identified the best fitted model with 1 numerical predictor and 1 categorical predictors (`critics_rating` with 2 levels), in total 3 predictors:

\[
  \begin{aligned}
  \widehat{logit(p)} = \hat{\beta_0} + \hat{\beta_1} \times (imdb\_rating) + \hat{\beta_2} \times (critics\_ratingFresh) + \hat{\beta_2} \times (critics\_ratingRotten) 
  \end{aligned}
\]
\

Where $\beta_i$ are the estimates listed below:

```{r echo=FALSE, layout="1-body-outset"}

kable(tidy(model_final)[1:4,])
```
\
The categorical variable `critics_rating` has the following levels:\

- `Certified Fresh`: Reference level
- `Fresh`: estimate $\hat{\beta_2} = -1.729583$
- `Rotten`: estimate $\hat{\beta_3} = -1.561766$

\
\

**Interpretation of the model coefficients**
\

The model coefficients explain the change in the logit output per unit of change in $x_i$, all others hold constant.\

Since logit values are not intuitive understandable it is better to explain the coefficient as "odds ratio (OR)", i.e. effect of change in odds. The odds ratios can be expressed by the exponent of the coefficient, $OR = \exp(\hat{\beta_i})$.
\

```{r}
# Calculate the ODDs ratios
options(scipen=0)

df_model <- tidy(model_final) %>% 
  filter(!term == "(Intercept)") %>% 
  mutate(odds_ratio = round(exp(estimate),3),
         significant = case_when(p.value < 0.05 ~ TRUE,
                                 p.value >= 0.05 ~ FALSE)) %>% 
  select(term, estimate, odds_ratio, significant)

kable(df_model[1:3,])
```
\

- For a one-unit increase of IMDb-rating, the odds of audience rating "Upright" increases by a factor of 45.9.\
\
- `critics_rating:Fresh` odds of being rated "Upright" by the audience are smaller by a factor of 0.18 than `critics_rating:Certified-Fresh` odds of being rated "Upright".\
\
- `critics_rating:Rotten` odds of being rated "Upright" by the audience are smaller by a factor of 0.21 than `critics_rating:Certified-Fresh` odds of being rated "Upright".\
\
\

**Answer the research questions**
\

*1. Can the binary (audience) rating be explained by attributes such as genre, other rating measures (e.g, IMDb and critics ratings), and nomination and awards (e.g. Academy Award-winning films)?*\ 
\

The binary variable `audience_rating` can be well explained by attributes `imdb_rating` and  `critics_rating`. The variables are statistically significant.\
\

*2. What is the most influential predictor for audience rating?*\

The overall most influential predictor for `audience_rating` is `imdb_rating`, i.e. low `imdb_rating` results in `audience_rating: Spilled` and high `imdb_rating` results in `audience_rating: Upright`. \
\
\

**Note**

The model describes only existing relationships between independent variables and Audience Rating. We cannot draw causation from these findings. Furthermore, we cannot use the model to make predictions for movie ratings that are outside of the time frame (1970 -2014). We also cannot infer the model to the popularity of movies in the general public, but only to data that are collected with the same variables and measures.
\
\

#### Summary Modeling

The best model was selected by a step-by-step forward selection approach using the AIC measure. 
The most influential variables are `imdb_rating` and `critics_rating`. The assumptions for the logistic regression modeling were met.

Overall, the logistic model is a reasonable estimate for `audience_rating` based on the given dataset. 
\
\
\

## 5. Prediction

**Fitting accuracy of the model using the test data**

To assess the quality of the model fit for the testing data we will plot the bucketed predicted probabilities against the bucketed observed probabilities with confidence intervals, as we did in  section 4. Again, we split the data into 10 buckets by quantiles of 10%.\

```{r message=FALSE, warning=FALSE}
# TEST DATA: Plot true response vs fitted values

ggplot() +
  geom_jitter(data = test1, aes(y = audience_rating, x = pred), 
              height = 0.03, alpha = 0.7, size = 2, color = "darkgrey")+
  geom_abline(data = probs3, aes(x = x_median), 
              slope = 1, color = "red", size = 0.5, linetype = 6 )+
  geom_pointrange(data = probs3, aes(x = x_median, y = p, ymin = lwr, ymax = upr), size = 0.7, alpha = 0.5)+
    labs(title = "True values vs predicted values - Test Data",
       subtitle = "Observed values with 95% confidence interval (for 10 buckets)")+
  xlab("Predicted Probability")+
  ylab("Observed Probability (Truth)")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))
```
\
\
The dashed line is within the confidence bound of 95% confidence intervals for most of the buckets. The three data points in the right upper corner with confidence interval 0 are not located directly on the diagonal line but are very close to it. In other words, looking at the graph we are 95% confident that more than 70% of the data points are expressed correctly by the model. We can therefore conclude that the logistic fit and the predicted values for the test data are reasonable.\
\
\

**Classification accuracy of the model**


Another way to assess the model performance is to look at their classification measures. A logistic regression will return predicted probabilities of the event occurrence. A threshold value, yet to be determined, is used as a criterion to divide the predicted probabilities into two classes {0, 1}.  If the outcome values are balanced the threshold value is usually set to 0.5. In this respect we can treat the logistic model as a classification problem.\ 
\

To evaluate a classification performance a decision matrix from the outcome is commonly used. From the decision matrix the metrics `accuracy`, `error rate`, `precision`, `sensitivity` and `specificity` can be calculated.\

The decision matrix (a.k.a. confusion matrix):

```{r echo=FALSE, fig.align='left', fig.show='hold', out.width="50%"}
knitr::include_graphics(c("~/R/StatRWeb/images/01_decision_matrix.png")
)
```

\

*Accuracy*, most intuitive, but problematic for imbalanced classes in response (e.g. 1 out of 1000)\
- *accuracy = (TP + TN) / (TP + FP + TN + FN)*
\

*Error Rate:* Opposite of *accuracy*. Shows how often the outcomes are being misclassified.\
- *error rate = (FP + FN) / (TP + FP + TN + FN)*
\

*Precision:* Ratio of true positive to the predicted positives. Used when the cost for false positive is high (i.e. avoid false classification of "Upright")\
- *precision = TP / (TP + FP)*
\

*Sensitivity:* Concerned about positive outcomes, and when the cost of false positive is low (i.e avoid false classification of "Spilled")\
- *sensitivity = TP / (TP + FN)*
\

*Specificity:* Concerned about negative outcomes and a high cost to a positive outcome. (i.e. in favor of outcome "Spilled", because outcome "Upright" is involved with high cost). \
- *specificity = TN / (TN + FP)*
\

For our purpose we will focus on the *accuracy* and *precision* metric, since the classes are evenly distributed. Below the decision matrices for the training and test data.\
\


```{r echo=FALSE, fig.align='left', fig.show='hold', out.width="40%", fig.cap="Decision matrix of classification results"}
knitr::include_graphics(
  c("~/R/StatRWeb/images/03_conf_matrix_train.png",
    "~/R/StatRWeb/images/04_conf_matrix_test.png"))
```

\

The classification metrics for test and training data are as follows:

```{r}
class_metrics <- class_metrics %>% 
  mutate(change = ifelse(metrics != "error", round((train_data-test_data)/train_data,3), "---" )
  )

kable(class_metrics)
```
\
The classification performance for the test data are in line with the the performance for the training dataset. We don't see any large decline in performance in the test data due to overfitting or underfitting. 

*Accuracy:* The prediction for `audience_rating` is 86.1% accurate for all cases using the test data. This is a 2.3% reduction in accuracy than for the training data as expected.\

*Precision:* The prediction of `audience_rating: Upright` is 88.3% correct compared to the total "Upright" predicted cases using the test data. This is a 2.6% reduction in precision than for the training data as expected.
\
\
\

#### Summary prediction
\

- The classification performance for the test data are in line with the the performance for the training data
- The classification accuracy for the correct `audience_rating` is 86.1%.
- The classification precision for the correct `audience_rating: Upright` is 88.3%.
- The overall accuracy score of the model for the classification from the test data is "good".\

\
\
\

## Summary and Outlook

For the project we used a dataset about movie ratings from Rotten Tomatos and IMDb. 

The task was to build a logistic regression model to identify the most influential of 7 variables that lead to the correct description of the binary variable audience rating ("Upright", "Spilled").

For the modeling the  original sample dataset was split into training data (80%) and test data (20%). The assumptions required for logistic regression were checked, and correlated variables and outlieres were removed. The best fitting model was build by a step-by-step forward selection approach using the AIC criteria. Two influential variables were identified: "imdb_rating" and "ciricis_rating". Especially, "imdb_rating" has the biggest impact on the outcome: 

I.e.: For a one-unit increase of IMDb-rating, the odds of audience rating "Upright" increases by a factor of 45.9. Whereas, for one unit increase of critics rating the odds of audience rating increases only by a factor of 0.2.

Finally, the model was tested for prediction and classification with the separated test data. The classification performance for the test data were in line with the performance with the training data. The overall classification scores achieved were good, with an accuracy of 86.1% and a precision of 88.3%

For future projects in logistic regression analysis the modeling could be further optimized by *cross-validation* approach for best fitting and prediction. Furthermore, one could use methods from *machine learning* to determine the threshold value for an optimized classification.\

\
\


## References:

\

- [OpenIntro Statistics: Fourth Edition](https://www.openintro.org/book/os/), by David Diez, Mine Cetinkaya-Rundel, Christoph Barr

- [Data Analysis with R Specialization](https://www.coursera.org/specializations/statistics), by Duke University on Coursera platform
      
      
      


