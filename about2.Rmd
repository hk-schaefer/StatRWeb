---
title: "Project 2: Step-by-Step Guide"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(statsr)
library(GGally)          # ------------------- extension of ggplot2
library(data.table)      # ------------------- table calculation
library(patchwork)       # ------------------- additional visualization tool
library(moments)         # ------------------- package to calculate Skewness
library(broom)           # ------------------- Convert stat obj. into tibbles
library(modeldata)       # ------------------- Sampling data for modeling
library(knitr)           # ------------------- support print of tables
```


```{r include=FALSE}
train_select <- readRDS("~/R/StatRWeb/data/train_select.Rdata")
verify_select <- readRDS("~/R/StatRWeb/data/verify_select.Rdata")
test_select <- readRDS("~/R/StatRWeb/data/test_select.Rdata")
model_eval_best <- readRDS("~/R/StatRWeb/data/model_eval_best.Rdata")
m_final <- readRDS("~/R/StatRWeb/data/m_final.Rdata")
```


# LINEAR REGRESSION
\
\
```{r echo=FALSE, fig.align='center', fig.show='hold', out.width="40%"}
knitr::include_graphics(c("~/R/StatRWeb/images/00_icon.jpg")
)
```
\


## 1. Introduction


**Objectives**

The objective of this project was to apply the learned contents of "Multiple linear regression and modeling". Skills demonstrated are:\

`Exploratory Data Analysis`, `Variable Selection`, `Multiple Linear Regression`, `Model Selection`,  `Inference`, `Prediction`, `Statistical Thinking`, `R programming`\
\
\

**Contents**

The multiple linear regression modeling project was split into the sections:

- Part 1: Data inspections
- Part 2: Research question
- Part 3: Explanatory data analysis (variable selection)
- Part 4: Modeling
- Part 5: Prediction

Code chunks and detailed descriptions can be found in the [**R-Markdown**](project2.html) file.
\
\

**Project Instruction**

Your boss has just acquired data about how much audiences and critics like movies as well as many other variables about the movies. The dataset includes information from [Rotten Tomatos](https://www.rottentomatoes.com/) and [IMDB](https://www.imdb.com/).

She is interested in learning what attributes make a movie popular. She is also interested in learning something new about movies. She wants your team to figure it all out.

As part of this project you will complete exploratory data analysis (EDA), modeling, and prediction.\
\
\

**The Dataset**

The dataset was provided by the course [**Data Analysis with R, Duke University**](https://www.coursera.org/specializations/statistics) hosted on the Coursera platform.\

The dataset is a sample set of 651 observations with 33 variables. The data were collected in the years from 1970 to 2014. The ratings were collected by [Rotten Tomatos] from the audience on voluntary basis and from selected groups of movie critics. So we can't claim that they were collected by random sampling. We, therefore, cannot infer the statistics to the general population, i.e. movie popularity for all people in the US. We can only infer the model on new data collected in the same time frame and under the same method as our sample dataset.\

A meaningful research question had to be formulated and answered by multiple linear regression and modeling.\
\
\

## 2. Research question

Can the Internet Movie Database (IMDb) rating be explained by factors such as:

a. type, genre, run-time, release timing, 
b. other rating measures (e.g, Tomato-measure from audience and critics), and
c. nomination and awards (e.g. Academy Award-winning films)?
\
\

**Task**

Build a multiple linear regression model to best fit the relationships between these variables optimized for a high prediction accuracy for new data.

The multiple regression model has the following linear form:

\[
  \begin{aligned}
  {y} = \beta{_0}\ + \beta{_1} x{_1} + \beta{_2} x{_2} + \beta{_3} x{_3} + . . . + \beta{_k} x{_k}
  \end{aligned}
\]

Where:\

- $y$ is the dependent or response variable\ 
- $\beta_i$ are the unknown coefficient of the linear model\
- $x_i$ are the independent variables\
- $k$ is the number of independent variables
\
\
\

## 3. Exploratory Data Analysis (EDA)
\

**Variable dependencies and relevance**

We identified the following variables of interest, which needed to be checked for linear relationship with the response variable, distribution and independence.\
\

Variable type                       | Variable name                   | Data type
------------------------------------|---------------------------------|------------- 
Dependent variable (response)       | imdb_response                   | numerical
                                    |                                 |
Independent variables (predictors)  | critics_score, audience_score   | numerical
Independent variables (predictors)  | runtime, imdb_num_votes         | numerical
Independent variables (predictors)  | title_type, mpaa_rating         | categorical                         
Independent variables (predictors)  | critics_rating, audience_rating | categorical
Independent variables (predictors)  | awards and nomination           | categorical
Independent variables (predictors)  | thtr_rel, dvd_rel (dates)       | date  

Ten non-relevant variables such as movie_title, studio, director, actors, and urls were dropped from the dataset.\
\
\

**Split data into training and test data**

For modeling (training) and testing (prediction) we split the dataset into three sub-sets:
training data (70%) for model training, verification data (20%) to verify prediction accuracy during the training, and testing data (10%) to check the prediction accuracy of the final model.\
\
\

**Check linear relationship of each predictor variable against response variable**

Each predictor variable was checked for distribution, outliers, linearity and correlation using histogram and boxplots  and single linear regression diagnostic plots.

Summary plot for numerical variables:

```{r echo=FALSE, fig.align='center', fig.show='hold', out.width="92%"}
knitr::include_graphics(c("~/R/StatRWeb/images/01_image.png")
)
```


Summary plots for categorical variables:


```{r echo=FALSE, fig.align='center', fig.show='hold', out.width="90%"}
knitr::include_graphics(c("~/R/StatRWeb/images/02_image.png")
)
knitr::include_graphics(c("~/R/StatRWeb/images/03_image.png")
)
```
\
\

In total 6 extreme outliers and influencial points had to be removed from the dataset.
Predictor `runtime` was truncated and log-transformed to meet a normal distribution. 
Several predictors had to be dropped due to non-linearity or low correlation  with the response variable.\
\
\

**Colinearity check (independence of variables)**


Finally, we had to check for independence between the predictor variables (colinearity). Independence is sometimes difficult to avoid, decisions have to be made out of contaxt and from knowledge of the data

For example, `audience_score` and `critics_score` are highly correlated. Nevertheless, they were independently collected from two different groups, audience and critics. Therefore, both were included in the model.

In contrast, `audience_rating` and `critics_rating` were dropped from the model, because they are just an categorical aggregation of `audience_score` and `critics_score` data, respectively.\
\
\

**Selected variables (pre-selection)**

Below the list of remaining important variables meeting the above conditions with information from **simple linear model** and diagnostic plots. As candidates for the model we selected 3 numerical predictors and 5 categorical predictors. In total 8 predictors.\


```{r echo=FALSE, fig.align='center', fig.show='hold', out.width="100%"}
knitr::include_graphics(c("~/R/StatRWeb/images/04_image.png")
)
```
\
\
\

## 4. Modeling

Our modeling goals were:

1. Finding the best model that fits the training dataset
2. Optimizing the model for high prediction accuracy
3. Avoiding over- or under-fitting, prefer the simplest and best model (parsimonious model)

The modeling was conducted by a step-by-step **forward selection** approach. As criteria measure for variable selection we used `adjusted R-squared` that optimizes the prediction accuracy. 

We run a prediction test with the verification dataset after each step and monitored its accuracy.\
\
\

**Forward selection approach**

We started with an empty model (model_null) and added variables one-at-a-time until we couldn't find any variables that improved the model as measured by adjusted R-squared. In this course we carried out each selection step manually step-by-step, resulting in 35 models.\

After we demonstrated the manual step-by-step process we utilized as alternative  the integrated `step()-function` from R package `MASS` instead.  The `step()-function` selects the best model by forward, backward or bi-directional approach with one function call.\

A list for the best models after each step is shown below, as well as a plot regarding improvements in fit, measured in adjusted R-squared (adjR2), and prediction accuracy, measured in Root Mean Square Error (RMSE).\

```{r echo=FALSE, layout="1-body-outset"}
kable(model_eval_best[,1:4])
```
\

```{r echo=FALSE, fig.asp=0.6, fig.width=7, message=FALSE, warning=FALSE}
p1 <- model_eval_best %>%
  ggplot(aes(x = model, y = adjusted_R2, group = 1))+
  geom_line()+
  geom_point()+
  ylim(0.76, 0.86)+
  labs(title = "Model selection by adj. R-squared",
       subtitle = "Higher is better")+
  xlab("")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

p2 <- model_eval_best %>%
  ggplot(aes(x = model, y = RMSE, group = 1))+
  geom_line()+
  geom_point()+
  ylim(0.45, 0.6)+
  labs(title = "RMSE from verification data",
       subtitle = "Lower is better")+
  xlab("")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))
p1 + p2
```
\
As result, model **m4_01** turned out to be the best model in respect of our goals (best fit, optimized prediction, simplest and best model)\
\
\


**Check conditions**

To apply inference to new datasets, the multiple regression method depends on the following assumptions:

1. the residuals are independent
2. the residuals are nearly normal distributed (less important for larger datasets)
3. the variability of the residuals is nearly constant
4. residuals are independent
5. each variable is linearly related to the response variable

```{r echo=FALSE, fig.asp=0.7, fig.width=7, message=FALSE, warning=FALSE}
train_select2 <- train_select
train_select2$fitted <- m_final$fitted.values
train_select2$residuals <- m_final$residuals

# linearity of residuals
p1 <- train_select2 %>% 
  ggplot(aes(x = fitted, y = residuals))+
  geom_point(size = 1, color = "black", alpha = 0.3)+
  geom_smooth(method = "loess", linetype = 1, se = F, color = "red")+
  geom_smooth(method = "lm", linetype = 1, se = F, color = "blue")+
  labs(title = "Model check: Linearity")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))


# histogram
p2 <- train_select2 %>% 
  ggplot(aes(x = residuals))+
  geom_histogram(binwidth = 0.1, fill = "darkgrey", col = "black")+
  labs(title = "Model check: Normal distribution")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))


# Homoscedasticity check
p3 <- train_select2 %>% 
  ggplot(aes(x = fitted, y = abs(residuals)))+
  geom_point(size = 1, color = "black", alpha = 0.3)+
  geom_smooth(method = "loess", linetype = 1, se = F, color = "red")+
  geom_smooth(method = "lm", linetype = 1, se = F, color = "blue")+
  labs(title = "Model check: Variability")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))


# Autocorrelation check: independence of residuals
p4 <- train_select2 %>% 
  ggplot(aes(x = thtr_rel_date, y = residuals))+
  geom_point(size = 1, color = "black", alpha = 0.3)+
  geom_smooth(method = "loess", linetype = 1, se = F, color = "red")+
  geom_smooth(method = "lm", linetype = 1, se = F, color = "blue")+
  labs(title = "Model check: Autocorrelation")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

(p1 + p2)/(p3 + p4)

```
\
\

```{r eval=FALSE, fig.asp=0.7, fig.width=7, message=FALSE, warning=FALSE, include=FALSE}
# Linearity check: audience_score
p1 <- train_select2 %>% 
  ggplot(aes(x = audience_score, y = residuals))+
  geom_point(size = 1, color = "black", alpha = 0.3)+
  geom_smooth(method = "loess", linetype = 1, se = F, color = "red")+
  geom_smooth(method = "lm", linetype = 1, se = F, color = "blue")+
  labs(title = "Variable check: Linearity - audience_score")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))


# Linearity check: critics_score
p2 <- train_select2 %>% 
  ggplot(aes(x = critics_score, y = residuals))+
  geom_point(size = 1, color = "black", alpha = 0.3)+
  geom_smooth(method = "loess", linetype = 1, se = F, color = "red")+
  geom_smooth(method = "lm", linetype = 1, se = F, color = "blue")+
  labs(title = "Variable check: Linearity - critics_score")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))


# Variability check: genres
train_select2 <- train_select2 %>% 
  mutate(genre2 = factor(case_when(
    genre == "Animation" ~ "Anima",
    genre == "Science Fiction & Fantasy" ~ "SF",
    genre == "Musical & Performing Arts" ~ "Musical",
    genre == "Action & Adventure" ~ "Action",
    genre == "Art House & International" ~ "Art",
    genre == "Mystery & Suspense" ~ "Mystery",
    .default = genre)
  ))

p3 <- train_select2 %>% 
  ggplot(aes(x = fct_reorder(genre2, residuals) , y = residuals))+
  geom_point(position = position_dodge(width = 2), color = "darkgrey", size = 4, alpha = 0.7)+
  geom_boxplot()+
  labs(title = "Variable check: Variability - genre")+
  xlab("genre")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))+
  coord_flip()

# Linearity check: log(runtime)
p4 <- train_select2 %>% 
  ggplot(aes(x = log(runtime), y = residuals))+
  geom_point(size = 1, color = "black", alpha = 0.3)+
  geom_smooth(method = "loess", linetype = 1, se = F, color = "red")+
  geom_smooth(method = "lm", linetype = 1, se = F, color = "blue")+
  labs(title = "Variable check: Linearity - log(runtime)")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

(p1 + p2)/(p3 + p4)

```
\

The conditions are largely met with some limitations;

- *Linearity*: A wavy non-linear component remains due to larger residuals on the left side of the x-axis.\
- *Constant variability (homoscedasticity)*: The residuals are related to the x-axis and show some funnel effect from right to left.\
\
\


#### Summary Modeling

The best model was selected by a step-by-step forward selection approach using adjusted R-squared measure. The model selection was further optimized by removing less significant predictors and for the best prediction accuracy.\ 


**The model**

\[
  \begin{aligned}
  \widehat{imdb\_rating} = \hat{\beta_0}\\
  &+ \hat{\beta_1} \times (critics\_score) +  \hat{\beta_2} \times (audience\_score) +
   \hat{\beta_3} \times (genreAnima) +  \hat{\beta_4} \times (genreArt)\\
  &+  \hat{\beta_5} \times (genreComedy) +  \hat{\beta_6} \times (genreDrama) +  \hat{\beta_7} \times (genreDocu) 
  +  \hat{\beta_8} \times  (genreHorror)\\
  &+  \hat{\beta_9} \times (genreMusical) +  \hat{\beta_{10}} \times (genreMystery) +  \hat{\beta_{11}} \times (genreOther) +
   \hat{\beta_{12}} \times (genreSF)\\
  &+  \hat{\beta_{13}} \times (runtime\_log)
  \end{aligned}
\]

Where $\beta_i$ are the estimates listed below:


```{r echo=FALSE, layout="1-body-outset"}
kable(tidy(summary(m_final))[1:14,])
```
\

Overall the linear model is a reasonable estimate for the given dataset. However, movies with lower IMDb ratings show a higher variability in critics and audience scores. For movies with mid or high ratings the variability is much smaller.\
\
\

**Answer the research questions**

**a) Can the Internet Movie Database (IMDb) rating be explained by title_type, genre, or runtime?**

The IMDb rating increases by 0.47 points per unit of log(runtime), i.e. the longer the movie the better the rating on average, when all other variables are hold constant.

Furthermore, the the IMDb rating increases by 0.32 points for genre "documentary", increases by 0.17 points for genre "mystery", decreases by 0.21 points for genre "comedy", and decreases by 0.28 points for genre "animation" on average, when all other variables are hold constant.
\
\

**b) Can the Internet Movie Database (IMDb) rating be explained by other rating measures, such as "Tomatomeasure" critics score or audience score?**

The IMDb rating increases by 0.03 points per unit audience score (scale 1 -100) and increases by 0.01 points per unit critics score (scale 1-100) on average, when all other variables are hold constant. I.e. audience score have a 3-time stronger relationship with IMDb rating than critics scores. 
\
\

**c) Can the Internet Movie Database (IMDb) rating be explained by Academy awards or nominations?**

Academy awards or nominations for best movie have a minor effect (increase by 0.08) on the IMDb rating but are not significant. Academy awards of best actor, actress or director have almost no impact and are not significant.
\
\

**Note**

The model describes only existing relationships between independent variables and IMDb rating. We cannot draw causation from these findings. Furthermore, we cannot use the model to make predictions for movie ratings that are outside of the time frame (1970 -2014). We also cannot infer the model to the popularity of movies in the general public, but only to data that are collected with the same variables and measures.
\
\
\

## 5. Prediction

As final step, we tested the model with a test dataset. The test dataset was separated from the original movies dataset, earlier in this project. Utilizing the final model we predicted predict the response variable (imdb_rating_hat) and compared it with its true value.

As measure of accuracy we used the Root Mean Square Error (RMSE). Finally,  we looked into residual plots to get some insight into their distribution and possible remaining patterns.
\

**Test dataset**

```{r echo=FALSE, fig.asp=0.7, fig.width=7, message=FALSE, warning=FALSE}
# Test data set (relationship)

# Distribution imdb_rating
p1 <- test_select %>% 
  ggplot(aes(x = imdb_rating)) +
  geom_histogram(binwidth = 0.5, fill = "darkgrey", col="black")+
  labs(title = "Test Data: Distribution",
       subtitle = "Sample size n = 72")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

# Relationship imdb_rating ~ audience_score
p2 <- test_select %>% 
  ggplot(aes(x = audience_score, y = imdb_rating))+
  geom_point(size = 2, color = "black", alpha = 0.5)+
  geom_smooth(method = "lm", se = F, color = "blue")+
  geom_smooth(method = "loess", se = F, color = "red")+
  labs(title = "IMDb Rating vs Audience Score",
       subtitle = "")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

# Relationship imdb_rating ~ critics_score
p3 <- test_select %>% 
  ggplot(aes(x = critics_score, y = imdb_rating))+
  geom_point(size = 2, color = "black", alpha = 0.5)+
  geom_smooth(method = "lm", se = F, color = "blue")+
  geom_smooth(method = "loess", se = F, color = "red")+
  labs(title = "IMDb Rating vs Critics Score",
       subtitle = "")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))


# Relationship imdb_rating ~ genre
p4 <- test_select %>% 
  ggplot(aes(y = imdb_rating, x = fct_reorder(genre, imdb_rating)))+
  geom_boxplot()+
  labs(title = "IMDb Rating vs Genre",
       subtitle = "")+
  xlab("genre")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))+
  coord_flip()

(p1 + p4)/(p2 + p3)

```
\
The test dataset has a sample size of 72 data points and are randomly distributed.\
\
\

#### Prediction and evaluation

**Root Mean Square Error (RMSE) comparison**

```{r include=FALSE}
# Predict from training data
train_predict <- train_select %>% 
  mutate(data.frame(predict(m_final, newdata = train_select, interval = "prediction")))

train_predict <- train_predict %>% 
  mutate(res = imdb_rating - fit,
         interval = (imdb_rating >= lwr & imdb_rating <= upr))

# Accuracy measure of model is mean of absolute percent error
rmse_train <- sqrt(sum(train_predict$res^2)/nrow(train_predict))

# print(paste0("RMSE training = ", round(rmse_train, 3)))
```

```{r include=FALSE}
# Predict from training data
test_predict <- test_select %>% 
  mutate(data.frame(predict(m_final, newdata = test_select, interval = "prediction")))

test_predict <- test_predict %>% 
  mutate(res = imdb_rating - fit,
         interval = (imdb_rating >= lwr & imdb_rating <= upr))

# Accuracy measure of model is mean of absolute percent error
rmse_test <- sqrt(sum(test_predict$res^2)/nrow(test_predict))

# print(paste0("RMSE test = ", round(rmse_test, 3)))

```
The RMSE for the test dataset is `r round(rmse_test,3)` and the RMSE of the training dataset is `r round(rmse_train,3)`. I.e. about 0.1 points larger than the RMSE of the training data set. This is expected because the model was fitted to the training dataset and not to the test dataset, hence the training RMSE is smaller.\
\
\

**Two examples: Interpretation of prediction and prediction interval**

Example 1: highest `imdb_rating`

```{r echo=FALSE, layout="1-body-outset"}
# Two examples: highest and lowest rating
kable(test_predict %>% 
  filter(imdb_rating == max(imdb_rating)) %>% 
  select(audience_score,
         critics_score,
         genre,
         runtime,
         imdb_rating,
         fit,
         res,
         lwr,
         upr,
         interval))
```

\
\[
  \begin{aligned}
  \widehat{imdb\_rating\_(9.0)} = \hat{\beta_{0}}\\
  &+ \hat{\beta_{1}} \times (97) + \hat{\beta_{2}} \times (97) + \hat{\beta_{3}} \times (0) + \hat{\beta_{4}} \times (0) + \hat{\beta_{5}} \times (0) + \hat{\beta_{6}} \times (0) + \hat{\beta_{7}} \times (0)\\ 
  &+ \hat{\beta_{8}} \times  (0) + \hat{\beta_{9}} \times (0) + \hat{\beta_{10}} \times (1) + \hat{\beta_{11}} \times (0) + \hat{\beta_{12}} \times (0) + \hat{\beta_{13}} \times (202\_log)
  \end{aligned}
\]

We are 95% sure that the average `imdb_rating` is between 7.7 and 9.3 points, for an `audience_score` of 97, `critics_score` of 97, `genre` "Mystery & Suspense" and `runtime` of 202 minutes. The true value is 9.0 points which is inside the interval and is a good prediction.\
\

Example 2: lowest `imdb_rating`

```{r echo=FALSE, layout="1-body-outset"}
# Two examples: highest and lowest rating
kable(test_predict %>% 
  filter(imdb_rating == min(imdb_rating)) %>% 
  select(audience_score,
         critics_score,
         genre,
         runtime,
         imdb_rating,
         fit,
         res,
         lwr,
         upr,
         interval))
```
\
\[
  \begin{aligned}
  \widehat{imdb\_rating\_(3.4)} = \hat{\beta_{0}}\\
  &+ \hat{\beta_{1}} \times (10) + \hat{\beta_{2}} \times (23) + \hat{\beta_{3}} \times (0) + \hat{\beta_{4}} \times (0) + \hat{\beta_{5}} \times (1) + \hat{\beta_{6}} \times (0) + \hat{\beta_{7}} \times (0)\\ 
  &+ \hat{\beta_{8}} \times  (0) + \hat{\beta_{9}} \times (0) + \hat{\beta_{10}} \times (0) + \hat{\beta_{11}} \times (0) + \hat{\beta_{12}} \times (0) + \hat{\beta_{13}} \times (85\_log)
  \end{aligned}
\]


We are 95% sure that the average `imdb_rating` is between 3.6 and 5.2 points, for an `audience_score` of 23, `critics_score` of 10, `genre` "Comedy" and `runtime` of 85 minutes. The true value, however, is 3.4 points which is outside the interval and is a bad prediction.\
\
\

**Number of data points that are outside of the prediction interval**


```{r warning=FALSE, include=FALSE}
total <- length(test_predict$fit)

test_predict %>% 
  group_by(interval) %>% 
  summarise(n = n(),
            percent = round(n/total *100, 1))
```

There are 6 points out of 72 (8.3%) that are outside the prediction interval, which is more than the expected rate of 5%. There are some effects that are not modeled correctly. On the other hand, about 92% were modeled correctly, which is a pretty good result.\
\
\

**Relationship between predicted and true values**

```{r echo=FALSE, fig.asp=0.7, fig.width=7, message=FALSE, warning=FALSE}

# Fitted vs actual values
test_predict %>% 
  ggplot(aes(x = imdb_rating))+
  geom_abline(slope = 1, intercept = 0, color = "black", size = 0.75, linetype = 6) +
  geom_pointrange(aes(y = fit, ymin = lwr, ymax = upr, color = interval), size = 0.5)+
  labs(title = "Predicted values vs true values - test data",
       subtitle = "Predicted values with prediction interval (95% confidence), n = 72")+
  ylim(3.,9.5)+
  xlim(3.,9.5)+
  xlab("imdb_rating (truth)")+
  ylab("imdb_rating (predicted)")+
  guides(color=guide_legend(title="In interval"))+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))+
  annotate("text", y=3.7, x=8.5, label="under estimated", size = 4)+
  annotate("text", y=9, x=4.7, label="over estimated", size = 4)+
  theme(legend.title=element_text("Test"))
```
\

In the ideal case all dots should be on the straight black line. However, the points, calculated from the model are offset from the ideal line. There are six points whose prediction intervals do not contain the true response value (red), predominantly in the lower range.\
\
\

**Diagnostics or prediction accuracy**

```{r echo=FALSE, fig.asp=0.5, fig.width=7, message=FALSE, warning=FALSE}
# Distribution of residuals
p1 <- test_predict %>% 
  ggplot(aes(x = res))+
  geom_histogram(binwidth = 0.2, fill="darkgrey", col="black") +
  labs(title = "Residual distribution - predicted test data",
       subtitle = "n = 72")+
  xlab("residuals")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

# Linearity check
p2 <- test_predict %>% 
  ggplot(aes(x = fit, y = abs(res)))+
  geom_point(size = 3, col = "black", alpha = 0.3)+
  geom_smooth(method = "lm", se = F)+
  geom_smooth(method = "loess", se = F, col = "red")+
  ylim(0, 2.5)+
  labs(title = "Residual variability of predictions",
       subtitle = "Absolute values")+
  xlab("imdb_rating (predicted)")+
  ylab("abs(residuals)")+
  theme(plot.title.position = "plot",
        axis.title.y = element_text(hjust=1),
        axis.title.x = element_text(hjust=0))

p1 + p2
```
\

Besides the few extreme values the distribution is nearly normal distributed. The residuals variability is slightly larger on the left (lower imdb ratings) due to some larger residuals. Some wavy pattern is still present. \
\
\


#### Summary Prediction

- We are 95% confident that 92% of the predicted data points contain the true response within their prediction interval, i.e. 92% of the predictions are correct.
- The wavy downward trend in the residual distribution suggests that there is still some underlying non-linear component present which could not be modeled. Refinement with a more general non-linear approach may lead to further improvements.
\
\
\


## References:

- [Data Analysis with R, Duke University](https://www.coursera.org/specializations/statistics) hosted on the Coursera platform\
- [R for Data Science (2e)](https://r4ds.hadley.nz/)\
\




